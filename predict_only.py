import torch
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast

label_map = {"ìœ„í—˜": 0, "ì£¼ì˜": 1, "ì•ˆì „": 2}
id2label = {v: k for k, v in label_map.items()}

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=3,
    id2label=id2label,
    label2id=label_map
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ğŸ“Ÿ ì‹¤í–‰ ì¥ì¹˜:", device)

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° (ê²½ë¡œ í™•ì¸ í•„ìš”)
try:
    model = DistilBertForSequenceClassification.from_pretrained("saved_model").to(device)
    tokenizer = DistilBertTokenizerFast.from_pretrained("saved_model")
    model.eval()
except Exception as e:
    print("ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:", e)
    exit()

def predict_url(url: str):
    if not url.strip():
        print("ì…ë ¥ê°’ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return None, None, None
    try:
        inputs = tokenizer(url, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
            
            probs = torch.softmax(outputs.logits, dim=1).squeeze()

            label = torch.argmax(probs).item()
            confidence = probs[label].item()
            label_str = {0: "ğŸ”´ ìœ„í—˜", 1: "ğŸŸ¡ ì£¼ì˜", 2: "ğŸŸ¢ ì•ˆì „"}.get(label, "ì•Œ ìˆ˜ ì—†ìŒ")
            print(f"\nğŸŒ {url}")
            print(f"â†’ {label_str} (ì‹ ë¢°ë„: {confidence:.2f})")
            return label, confidence, label_str
    except Exception as e:
        print("ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)
        return None, None, None

# ì…ë ¥ ë£¨í”„
while True:
    user_input = input("\nURL ì…ë ¥ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ").strip()
    if user_input.lower() == "exit":
        break
    predict_url(user_input)

