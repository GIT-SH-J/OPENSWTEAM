import torch
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
from urllib.parse import urlparse, urlunparse

# === [0] ì¥ì¹˜ ì„¤ì • ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ğŸ“Ÿ ì‹¤í–‰ ì¥ì¹˜:", device)

# === [1] ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° ===
model = DistilBertForSequenceClassification.from_pretrained("saved_model").to(device)
tokenizer = DistilBertTokenizerFast.from_pretrained("saved_model")

# === [2] URL ì •ê·œí™” í•¨ìˆ˜ ===
def normalize_url(url):
    parsed = urlparse(url.strip())
    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))

# === [3] ì˜ˆì¸¡ í•¨ìˆ˜ ===
def predict_url(url: str):
    # 1) ì •ê·œí™”
    norm_url = normalize_url(url)
    
    # 2) ì…ë ¥ í…ì„œí™”
    model.eval()
    inputs = tokenizer(norm_url, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # 3) ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1).squeeze()
        label = torch.argmax(probs).item()
        confidence = probs[label].item()
        
        # 4) í™•ì‹  ë‚®ì€ ìœ„í—˜ì€ ì£¼ì˜ë¡œ ì™„í™”
        if label == 0 and confidence < 0.85:
            label = 1
        
        label_str = {0: "ğŸ”´ ìœ„í—˜", 1: "ğŸŸ¡ ì£¼ì˜", 2: "ğŸŸ¢ ì•ˆì „"}[label]
        print(f"\nğŸŒ ì›ë³¸ URL: {url}")
        print(f"ğŸ”§ ì •ê·œí™”ëœ URL: {norm_url}")
        print(f"â†’ ì˜ˆì¸¡ ê²°ê³¼: {label_str} (ì‹ ë¢°ë„: {confidence:.2f})")

# === [4] ì‚¬ìš©ì ì…ë ¥ ë£¨í”„ ===
while True:
    user_input = input("\nURL ì…ë ¥ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
    if user_input.lower() == "exit":
        break
    predict_url(user_input)
