import torch
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
from urllib.parse import urlparse, unquote, parse_qs
import csv
from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import os
import io

# PE/SH íŒŒì¼ ë¶„ì„ì„ ìœ„í•œ ìƒˆë¡œìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pefile
import bashlex
import re
import base64
import sys

# decoder/base64_decoder.py ì—ì„œ í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
# sys.path.appendë¥¼ ì‚¬ìš©í•˜ë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ ì‹œì—ë„
# 'decoder' íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '.'))) # í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€

try:
    # ì´ ì„í¬íŠ¸ ë¬¸ì€ 'your_project_folder/decoder/base64_decoder.py' ì— í•´ë‹¹ í•¨ìˆ˜ë“¤ì´ ìˆì„ ë•Œ ìœ íš¨í•©ë‹ˆë‹¤.
    from decoder.base64_decoder import find_base64_block, decode_base64_string, decode_obfuscated_url_preserve_query
except ImportError as e:
    print(f"ë””ì½”ë” ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜: {e}")
    print("'decoder/base64_decoder.py' íŒŒì¼ì´ ì¡´ì¬í•˜ë©° 'find_base64_block', 'decode_base64_string', 'decode_obfuscated_url_preserve_query' í•¨ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    # ì„ì‹œ ë”ë¯¸ í•¨ìˆ˜. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ ë””ì½”ë” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê±°ë‚˜ ì—ëŸ¬ ì²˜ë¦¬ í•„ìš”.
    def find_base64_block(text: str) -> str:
        match = re.search(r'(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?', text)
        return match.group(0) if match else ""
    def decode_base64_string(b64_str: str) -> str:
        try:
            return base64.b64decode(b64_str + '==').decode('utf-8', errors='ignore') # íŒ¨ë”© ë³´ì •
        except Exception:
            return ""
    def decode_obfuscated_url_preserve_query(url: str) -> str:
        try:
            # ê¸°ë³¸ URL ì¸ì½”ë”© í•´ì œ
            decoded_url = unquote(url)
            
            # Base64 íŒ¨í„´ì„ ì°¾ì•„ ë””ì½”ë”© ì‹œë„ (ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ë„ ìˆìŒ)
            b64_block = find_base64_block(decoded_url)
            if b64_block:
                base64_decoded = decode_base64_string(b64_block)
                # Base64 ë””ì½”ë”© ê²°ê³¼ê°€ ìœ íš¨í•œ URL í˜•íƒœë¼ë©´, í•´ë‹¹ URLì„ ë°˜í™˜
                if base64_decoded and (base64_decoded.startswith('http://') or base64_decoded.startswith('https://')):
                    return base64_decoded
            return decoded_url
        except Exception:
            return url

app = FastAPI()

# CORS ì„¤ì • (ì´ì „ê³¼ ë™ì¼)
origins = [
    "http://localhost",
    "http://localhost:8000",
    "http://127.0.0.1:8000",
    "http://127.0.0.1:5500", # VS Code Live Serverë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
    "null"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì¥ì¹˜ ì„¤ì • (ì´ì „ê³¼ ë™ì¼)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ì‹¤í–‰ ì¥ì¹˜:", device)

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° (ì´ì „ê³¼ ë™ì¼)
model = None
tokenizer = None
model_path = "saved_model"
if os.path.exists(model_path):
    try:
        model = DistilBertForSequenceClassification.from_pretrained(model_path).to(device)
        tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)
        print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"'{model_path}' ë””ë ‰í† ë¦¬ê°€ '{os.getcwd()}'ì— ìˆëŠ”ì§€, ê·¸ë¦¬ê³  ìœ íš¨í•œ ëª¨ë¸ íŒŒì¼ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
else:
    print(f"ì˜¤ë¥˜: '{model_path}' ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# Majestic Millionì—ì„œ ì•ˆì „ ë„ë©”ì¸ ë¡œë”© (ì´ì „ê³¼ ë™ì¼)
safe_domains = set()
majestic_path = "majestic_million.csv"
if os.path.exists(majestic_path):
    try:
        with open(majestic_path, newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)
            for row in reader:
                if len(row) > 2:
                    domain = row[2].strip().lower()
                    if domain:
                        safe_domains.add(domain)
        print(f"ì•ˆì „ ë„ë©”ì¸ {len(safe_domains)}ê°œ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"majestic_million.csv ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
else:
    print(f"ì˜¤ë¥˜: '{majestic_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•ˆì „ ë„ë©”ì¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# URL ì •ê·œí™” ë° ë„ë©”ì¸ ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)
def get_domain(url):
    try:
        parsed = urlparse(url)
        netloc = parsed.netloc
        if netloc.startswith("www."):
            netloc = netloc[4:]
        return netloc.lower()
    except Exception:
        return ""

# === í•µì‹¬ URL ì˜ˆì¸¡ ë¡œì§ í•¨ìˆ˜ (ìˆ˜ì •ë¨) ===
def process_single_url(url: str):
    if model is None or tokenizer is None:
        return {"success": False, "message": "ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    norm_url = url.strip()
    if not norm_url:
        return {"success": True, "label_str": "ì•Œ ìˆ˜ ì—†ìŒ", "confidence": 0.0, "original_url": url, "decoded_url": "ë¹ˆ URL"}

    decoded_url_for_analysis = norm_url

    try:
        # 1. ì¼ë‹¨ ì „ì²´ URLì— ëŒ€í•´ decode_obfuscated_url_preserve_queryë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
        #    ì´ëŠ” URL ìì²´ì— URL ì¸ì½”ë”©/Base64 ì¸ì½”ë”©ì´ ë˜ì–´ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
        temp_decoded_url = decode_obfuscated_url_preserve_query(norm_url)
        
        # 2. ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ 'url' ë˜ëŠ” ìœ ì‚¬í•œ ì´ë¦„ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ ë””ì½”ë”©ì„ ì‹œë„í•©ë‹ˆë‹¤.
        parsed_url = urlparse(temp_decoded_url)
        query_params = parse_qs(parsed_url.query)

        target_param_keys = ['url', 'redirect', 'target', 'link', 'data'] # íƒìƒ‰í•  íŒŒë¼ë¯¸í„° í‚¤
        found_obfuscated_param = False

        for key in target_param_keys:
            if key in query_params:
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë˜ë¯€ë¡œ ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
                param_value = query_params[key][0] 
                
                # íŒŒë¼ë¯¸í„° ê°’ì— ëŒ€í•´ ë‹¤ì‹œ ë‚œë…í™” í•´ì œ ì‹œë„
                decoded_param_value = decode_obfuscated_url_preserve_query(param_value)
                
                # ë””ì½”ë”©ëœ ê°’ì´ ìœ íš¨í•œ URLì²˜ëŸ¼ ë³´ì¸ë‹¤ë©´, ì´ URLì„ ìµœì¢… ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©
                if decoded_param_value and (decoded_param_value.startswith('http://') or decoded_param_value.startswith('https://')):
                    decoded_url_for_analysis = decoded_param_value
                    found_obfuscated_param = True
                    break # ì°¾ì•˜ìœ¼ë©´ ë” ì´ìƒ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì§€ ì•ŠìŒ
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ ë‚œë…í™”ëœ URLì„ ì°¾ì§€ ëª»í–ˆê³ ,
        # ì›ë˜ URLì´ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆë¥¼ ê°€ì§€ê³  ìˆë‹¤ë©´, ìµœì´ˆì˜ ë””ì½”ë”©ëœ URL (temp_decoded_url)ì„ ì‚¬ìš©
        if not found_obfuscated_param:
             decoded_url_for_analysis = temp_decoded_url

    except Exception as e:
        print(f"URL íŒŒì‹±/ë””ì½”ë”© ì˜¤ë¥˜ for '{norm_url}': {e}")
        decoded_url_for_analysis = norm_url # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ì‚¬ìš©

    domain = get_domain(decoded_url_for_analysis)

    if domain and domain in safe_domains:
        return {"success": True, "label_str": "ğŸŸ¢ ì•ˆì „ (ë„ë©”ì¸ ì‹ ë¢° ê¸°ë°˜)", "confidence": 1.0, "original_url": url, "decoded_url": decoded_url_for_analysis}

    model.eval()
    inputs = tokenizer(decoded_url_for_analysis, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1).squeeze()
        label = torch.argmax(probs).item()
        confidence = probs[label].item()

        if label == 0 and confidence < 0.85:
            label = 1

        label_map = {0: "ğŸ”´ ìœ„í—˜", 1: "ğŸŸ¡ ì£¼ì˜", 2: "ğŸŸ¢ ì•ˆì „"}
        label_str = label_map.get(label, "ì•Œ ìˆ˜ ì—†ìŒ")

        return {"success": True, "label_str": label_str, "confidence": confidence, "original_url": url, "decoded_url": decoded_url_for_analysis}

# ìš”ì²­ ë°”ë””ë¥¼ ìœ„í•œ Pydantic ëª¨ë¸ ì •ì˜ (ì´ì „ê³¼ ë™ì¼)
class URLRequest(BaseModel):
    url: str

# ë‹¨ì¼ URL ì˜ˆì¸¡ API ì—”ë“œí¬ì¸íŠ¸ (ì´ì „ê³¼ ë™ì¼)
@app.post("/predict")
async def predict_url_api(request: URLRequest):
    result = process_single_url(request.url)
    if not result["success"] and result["message"] == "ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.":
        raise HTTPException(status_code=500, detail=result["message"])
    return result

# PE íŒŒì¼ ë¶„ì„ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)
def analyze_pe_file(file_content_bytes: bytes):
    extracted_urls = []
    pe_header_info = {}
    extracted_strings = []

    try:
        pe = pefile.PE(data=file_content_bytes)

        pe_header_info = {
            "Machine": hex(pe.FILE_HEADER.Machine),
            "NumberOfSections": pe.FILE_HEADER.NumberOfSections,
            "TimeDateStamp": pe.FILE_HEADER.TimeDateStamp,
            "Characteristics": hex(pe.FILE_HEADER.Characteristics),
            "Subsystem": hex(pe.OPTIONAL_HEADER.Subsystem),
            "DllCharacteristics": hex(pe.OPTIONAL_HEADER.DllCharacteristics),
            "ImageBase": hex(pe.OPTIONAL_HEADER.ImageBase),
            "EntryPoint": hex(pe.OPTIONAL_HEADER.AddressOfEntryPoint),
        }

        for section in pe.sections:
            try:
                data = section.get_data()
                ascii_strings = re.findall(rb'[ -~]{5,}', data)
                decoded_ascii_strings = [s.decode(errors='ignore') for s in ascii_strings]
                extracted_strings.extend(decoded_ascii_strings)

                for s in decoded_ascii_strings:
                    b64_str = find_base64_block(s)
                    if b64_str:
                        decoded_val = decode_base64_string(b64_str)
                        if decoded_val and urlparse(decoded_val).scheme in ['http', 'https']:
                            extracted_urls.append(decoded_val)
            except Exception as e:
                continue

    except pefile.PEFormatError as e:
        print(f"PE íŒŒì¼ í¬ë§· ì˜¤ë¥˜: {e}")
        return {"error": f"PE íŒŒì¼ í¬ë§· ì˜¤ë¥˜: {e}"}
    except Exception as e:
        print(f"PE íŒŒì¼ ë¶„ì„ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return {"error": f"PE íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}"}

    return {
        "pe_header_info": pe_header_info,
        "extracted_strings": list(set(extracted_strings)),
        "extracted_urls": list(set(extracted_urls))
    }

# SH íŒŒì¼ ë¶„ì„ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)
URL_PATTERN = re.compile(r'https?://[^\s\'\"\\|`]+')

def extract_urls_from_text(text):
    return URL_PATTERN.findall(text)

def decode_command_substitution(val, variables):
    inner = val.strip()
    if inner.startswith('$(') and inner.endswith(')'):
        inner_cmd = inner[2:-1].strip()
        parts = inner_cmd.split('|')
        if len(parts) >= 2:
            left = parts[0].strip()
            right = parts[1].strip()

            if left.startswith('echo '):
                echo_content = left[5:].strip().strip('"').strip("'")
                for var, valv in variables.items():
                    echo_content = echo_content.replace(f"${{{var}}}", valv).replace(f"${var}", valv)
                
                if 'base64' in right:
                    decoded_val = decode_base64_string(echo_content)
                    return decoded_val
    return None

def analyze_sh_script(script_content: str):
    variables = {}
    urls_found = set()

    for url in extract_urls_from_text(script_content):
        urls_found.add(url)

    try:
        parts = bashlex.parse(script_content)
    except Exception as e:
        for line in script_content.split('\n'):
            line = line.strip()
            if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*=".*"$', line) or re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*=\$\(.+\)$', line):
                try:
                    key, val_part = line.split('=', 1)
                    key = key.strip()
                    val = val_part.strip().strip('"').strip("'")
                    variables[key] = val

                    b64_block = find_base64_block(val)
                    if b64_block:
                        decoded = decode_base64_string(b64_block)
                        if decoded:
                            for url in extract_urls_from_text(decoded):
                                urls_found.add(url)
                    
                    if val.startswith('$(') and val.endswith(')'):
                        decoded_cmd_sub = decode_command_substitution(val, variables)
                        if decoded_cmd_sub:
                            variables[key] = decoded_cmd_sub
                            for url in extract_urls_from_text(decoded_cmd_sub):
                                urls_found.add(url)
                except ValueError:
                    pass
        return list(urls_found)

    def recursive_walk(node):
        nonlocal variables, urls_found

        if node.kind == 'assignment' and len(node.parts) >= 2:
            key = node.parts[0].word
            val_node = node.parts[1]
            val = ''
            if val_node.kind == 'word':
                val = val_node.word.strip('"').strip("'")
                variables[key] = val
            
                b64_block = find_base64_block(val)
                if b64_block and ('$' not in val) and ('${' not in val):
                    decoded = decode_base64_string(b64_block)
                    if decoded:
                        variables[key] = decoded
                        for url in extract_urls_from_text(decoded):
                            urls_found.add(url)

            elif val_node.kind in ['word', 'command_substitution']:
                val = val_node.word if hasattr(val_node, 'word') else ''
                decoded = decode_command_substitution(val, variables)
                if decoded:
                    variables[key] = decoded
                    for url in extract_urls_from_text(decoded):
                        urls_found.add(url)

        elif node.kind == 'command' and node.parts:
            first_part_word = node.parts[0].word if hasattr(node.parts[0], 'word') else ''
            if '=' in first_part_word and not first_part_word.startswith('$'):
                try:
                    key, val_part = first_part_word.split('=', 1)
                    val = val_part.strip('"').strip("'")
                    variables[key] = val
                    b64_block = find_base64_block(val)
                    if b64_block and ('$' not in val) and ('${' not in val):
                        decoded = decode_base64_string(b64_block)
                        if decoded:
                            variables[key] = decoded
                            for url in extract_urls_from_text(decoded):
                                urls_found.add(url)
                    return
                except ValueError:
                    pass

            args = []
            for p in node.parts[1:]:
                if hasattr(p, 'word'):
                    args.append(p.word)

            resolved_args = []
            for arg in args:
                for var, valv in variables.items():
                    arg = arg.replace(f"${{{var}}}", valv).replace(f"${var}", valv)
                resolved_args.append(arg)
            
            for arg in resolved_args:
                if ('$' not in arg) and ('${' not in arg):
                    b64_block = find_base64_block(arg)
                    if b64_block:
                        decoded = decode_base64_string(b64_block)
                        if decoded:
                            for url in extract_urls_from_text(decoded):
                                urls_found.add(url)
                for url in extract_urls_from_text(arg):
                    urls_found.add(url)

        if hasattr(node, 'parts'):
            for part in node.parts:
                recursive_walk(part)
        if hasattr(node, 'commands'):
            for command in node.commands:
                recursive_walk(command)
        if hasattr(node, 'body'):
            for part in node.body:
                recursive_walk(part)

    for part in parts:
        recursive_walk(part)

    return list(urls_found)

# íŒŒì¼ ì—…ë¡œë“œ ì˜ˆì¸¡ API ì—”ë“œí¬ì¸íŠ¸ (ì´ì „ê³¼ ë™ì¼)
@app.post("/predict_file")
async def predict_file_api(file: UploadFile = File(...)):
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì €ê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    file_extension = os.path.splitext(file.filename)[1].lower()
    file_content_bytes = await file.read()
    
    analysis_results = {}
    extracted_urls_to_classify = []
    file_type = "unknown"

    if file_extension in ['.exe', '.dll', '.sys', '.ocx', '.scr', '.cpl', '.drv']:
        file_type = "pe"
    elif file_extension == '.sh':
        file_type = "sh"
    else:
        if file.content_type == "application/x-msdownload" or file.content_type == "application/octet-stream":
             if file_content_bytes[:2] == b'MZ':
                 file_type = "pe"
        elif file.content_type == "application/x-sh" or file.content_type == "text/plain":
            if file_content_bytes.startswith(b'#!') and b'sh' in file_content_bytes[:10]:
                file_type = "sh"
            
    if file_type == "pe":
        analysis_results = analyze_pe_file(file_content_bytes)
        if "error" in analysis_results:
            raise HTTPException(status_code=400, detail=f"PE íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {analysis_results['error']}")
        extracted_urls_to_classify = analysis_results.get("extracted_urls", [])
        
    elif file_type == "sh":
        try:
            script_content = file_content_bytes.decode('utf-8', errors='ignore')
            extracted_urls_to_classify = analyze_sh_script(script_content)
            analysis_results = {"extracted_urls": extracted_urls_to_classify, "script_content_preview": script_content[:500]}
        except UnicodeDecodeError:
            raise HTTPException(status_code=400, detail="SH íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜. UTF-8ë¡œ ì¸ì½”ë”©ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"SH íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    else:
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PE íŒŒì¼(.exe, .dll ë“±) ë˜ëŠ” SH íŒŒì¼(.sh)ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í™•ì¥ì: {file_extension}, Content-Type: {file.content_type})")

    classified_urls = []
    if extracted_urls_to_classify:
        for url in extracted_urls_to_classify:
            classification_result = process_single_url(url)
            classified_urls.append(classification_result)
    
    final_response = {
        "success": True,
        "file_type": file_type,
        "filename": file.filename,
        "analysis_summary": analysis_results,
        "classified_urls": classified_urls
    }
    return final_response

# API ì„œë²„ì˜ ë£¨íŠ¸ ê²½ë¡œ í™•ì¸ (ì´ì „ê³¼ ë™ì¼)
@app.get("/")
async def read_root():
    return {"message": "URL Classification API is running!"}